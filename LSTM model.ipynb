{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Model for Social Media Analysis"
      ],
      "metadata": {
        "id": "KsxTdLtzTTOn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part, we use LSTM neural network to analyze Twitter data. In the final practical test, the effect is not bad."
      ],
      "metadata": {
        "id": "wUOQZyNp1yLt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Basic Library"
      ],
      "metadata": {
        "id": "-nmNUWXBTZHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import torchtext\n",
        "from torchtext.legacy import data\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "vHLmH9gKTeZJ"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning Data"
      ],
      "metadata": {
        "id": "4SRAKxvrmT-R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Delete non-utf8 encoding block and save the csv file"
      ],
      "metadata": {
        "id": "QvkjQoyzmcBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/BigData/ Sentiment140_dataset.csv', 'rb') as csv_in:\n",
        "    with open('newdataset.csv', \"w\", encoding=\"utf-8\") as csv_temp:\n",
        "        for line in csv_in:\n",
        "            if not line:\n",
        "                break\n",
        "            else:\n",
        "                line = line.decode(\"utf-8\", \"ignore\")\n",
        "                csv_temp.write(str(line).rstrip() + '\\n')"
      ],
      "metadata": {
        "id": "jbwkg_sMmXlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Find Nan value"
      ],
      "metadata": {
        "id": "I02MLzC8mi72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(r'/content/drive/MyDrive/Colab Notebooks/BigData/newdataset.csv')\n",
        "print(df.isnull().any())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F849FsZHmnqK",
        "outputId": "21c91912-7215-4dd4-9f8d-72ad7aa530d3"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0                                                                                                                      False\n",
            "1467810369                                                                                                             False\n",
            "Mon Apr 06 22:19:45 PDT 2009                                                                                           False\n",
            "NO_QUERY                                                                                                               False\n",
            "_TheSpecialOne_                                                                                                        False\n",
            "@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D    False\n",
            "dtype: bool\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Delete Nan and save csv file"
      ],
      "metadata": {
        "id": "UTqcjsBQm9D1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna()\n",
        "df.to_csv('Sentiment140DataSet_clean.csv')"
      ],
      "metadata": {
        "id": "WGuWdeNUnFqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "23t_wUB2TiTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = '/content/drive/MyDrive/Colab Notebooks/BigData/Sentiment140DataSet_clean.csv'\n",
        "df = pd.read_csv(input_file, header = None)"
      ],
      "metadata": {
        "id": "CPfyUpn1TkYL"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# View Dataset"
      ],
      "metadata": {
        "id": "WwKs6TTdUmaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0e5t3R_UoqH",
        "outputId": "5535f82f-9430-460a-9036-7f9fc8efe83a"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  \\\n",
            "0        0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
            "1        0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
            "2        0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
            "3        0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
            "4        0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY   \n",
            "...     ..         ...                           ...       ...   \n",
            "1599994  4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
            "1599995  4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
            "1599996  4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
            "1599997  4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
            "1599998  4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
            "\n",
            "         _TheSpecialOne_  \\\n",
            "0          scotthamilton   \n",
            "1               mattycus   \n",
            "2                ElleCTF   \n",
            "3                 Karoli   \n",
            "4               joy_wolf   \n",
            "...                  ...   \n",
            "1599994  AmandaMarie1028   \n",
            "1599995      TheWDBoards   \n",
            "1599996           bpbabe   \n",
            "1599997     tinydiamondz   \n",
            "1599998   RyanTrevMorris   \n",
            "\n",
            "        @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
            "0        is upset that he can't update his Facebook by ...                                                                   \n",
            "1        @Kenichan I dived many times for the ball. Man...                                                                   \n",
            "2          my whole body feels itchy and like its on fire                                                                    \n",
            "3        @nationwideclass no, it's not behaving at all....                                                                   \n",
            "4                            @Kwesidei not the whole crew                                                                    \n",
            "...                                                    ...                                                                   \n",
            "1599994  Just woke up. Having no school is the best fee...                                                                   \n",
            "1599995  TheWDB.com - Very cool to hear old Walt interv...                                                                   \n",
            "1599996  Are you ready for your MoJo Makeover? Ask me f...                                                                   \n",
            "1599997  Happy 38th Birthday to my boo of alll time!!! ...                                                                   \n",
            "1599998  happy #charitytuesday @theNSPCC @SparksCharity...                                                                   \n",
            "\n",
            "[1599999 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoqaykqnUp8i",
        "outputId": "822b3904-c307-4250-98a5-f5fb7a3cff32"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1599999 entries, 0 to 1599998\n",
            "Data columns (total 6 columns):\n",
            " #   Column                                                                                                               Non-Null Count    Dtype \n",
            "---  ------                                                                                                               --------------    ----- \n",
            " 0   0                                                                                                                    1599999 non-null  int64 \n",
            " 1   1467810369                                                                                                           1599999 non-null  int64 \n",
            " 2   Mon Apr 06 22:19:45 PDT 2009                                                                                         1599999 non-null  object\n",
            " 3   NO_QUERY                                                                                                             1599999 non-null  object\n",
            " 4   _TheSpecialOne_                                                                                                      1599999 non-null  object\n",
            " 5   @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  1599999 non-null  object\n",
            "dtypes: int64(2), object(4)\n",
            "memory usage: 73.2+ MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the Category and Index"
      ],
      "metadata": {
        "id": "wVRNh0AaUuh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"sentiment_category\"] = df[0].astype('category')\n",
        "df[\"sentiment_category\"]\n",
        "\n",
        "df[\"sentiment\"] = df[\"sentiment_category\"].cat.codes\n",
        "print(df[\"sentiment\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlHZWSHHUxlL",
        "outputId": "96667486-ff80-43fb-c20c-fa10563c8a07"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0               -1\n",
            "1                0\n",
            "2                1\n",
            "3                2\n",
            "4                3\n",
            "            ...   \n",
            "1599995    1599994\n",
            "1599996    1599995\n",
            "1599997    1599996\n",
            "1599998    1599997\n",
            "1599999    1599998\n",
            "Name: sentiment, Length: 1600000, dtype: int32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-process the Dataset"
      ],
      "metadata": {
        "id": "XFiMTUnRVfBT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Create labels and fields for the dataset"
      ],
      "metadata": {
        "id": "_5DgvEPVY19L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LABEL = data.LabelField()\n",
        "TWEET = data.Field(tokenize='spacy', tokenizer_language = 'en_core_web_sm', lower = True)"
      ],
      "metadata": {
        "id": "kL_yVnwUVkjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fields = [('score',None), ('id',None), ('date',None), ('query',None), ('name',None),  ('tweet', TWEET),('category',None),('label',LABEL)]\n",
        "\n",
        "print(fields)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GT8TmfXXVnXM",
        "outputId": "3b885e75-34f3-4bc0-8460-f9e52515410e"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('score', None), ('id', None), ('date', None), ('query', None), ('name', None), ('tweet', <torchtext.legacy.data.field.Field object at 0x7f4b7c8be310>), ('category', None), ('label', <torchtext.legacy.data.field.LabelField object at 0x7f4b7c8be460>)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Create Dataset and Split the Dataset"
      ],
      "metadata": {
        "id": "ybsa1B0ZZIXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "twitterDataset = data.dataset.TabularDataset(\n",
        "        path = \"/content/drive/MyDrive/Colab Notebooks/BigData/Sentiment140DataSet_clean.csv\", \n",
        "        format = \"CSV\", \n",
        "        fields = fields,\n",
        "        skip_header = False)"
      ],
      "metadata": {
        "id": "Gr4duy0mZPD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train, validation, test) = twitterDataset.split(\n",
        "                            split_ratio = [0.8,0.1,0.1]\n",
        "                            )\n",
        "\n",
        "print(\"Length of the Training Set: \" ,len(train))\n",
        "print(\"Length of the Validation Set: \" ,len(validation))\n",
        "print(\"Length of the Testing Set: \" ,len(test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xhfc1gcvZe-a",
        "outputId": "c2e24947-0a8e-4e31-8e8a-b76301969ba6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of the Training Set:  1280000\n",
            "Length of the Validation Set:  160000\n",
            "Length of the Testing Set:  160000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To Build a Vocabulary and Display"
      ],
      "metadata": {
        "id": "W0KwD4OwZ9Hm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 300000\n",
        "TWEET.build_vocab(train, max_size = vocab_size)\n",
        "\n",
        "\n",
        "print(\"Length of the vocabulary: \" ,len(TWEET.vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siMj8XavaSZi",
        "outputId": "9e7c5e31-e18f-4dad-b6ec-7030f47fa54d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of the vocabulary:  300002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create DataLoader"
      ],
      "metadata": {
        "id": "yRT9BTX7gT0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader, valid_dataloader, test_dataloader = data.BucketIterator.splits(\n",
        "    (train, validation, test),\n",
        "    batch_size = 32,\n",
        "    sort_key = lambda x: len(x.tweet),\n",
        "    sort_within_batch = False)"
      ],
      "metadata": {
        "id": "vq2dCydwgWwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create LSTM Model"
      ],
      "metadata": {
        "id": "i3nww2MZgeuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sentiment_LSTMModel(nn.Module):\n",
        "    def __init__(self, hidden_size, embedding_dim, vocab_size):\n",
        "        super(Sentiment_LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.encoder = nn.LSTM(input_size=embedding_dim,\n",
        "        hidden_size=hidden_size, num_layers=1)\n",
        "        self.predictor = nn.Linear(hidden_size, 2)\n",
        "        pass\n",
        "    \n",
        "    def forward(self, seq):\n",
        "        output, (hidden,_) = self.encoder(self.embedding(seq))\n",
        "        preds = self.predictor(hidden.squeeze(0))\n",
        "        return preds\n",
        "        pass"
      ],
      "metadata": {
        "id": "apLgLofcgglo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sentiment_LSTMModel(hidden_size = 100,embedding_dim = 300, vocab_size = vocab_size)"
      ],
      "metadata": {
        "id": "lnF29MLYgpYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Loss Function and Optimizer"
      ],
      "metadata": {
        "id": "tG1WSF-Mgt2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-2)"
      ],
      "metadata": {
        "id": "s05u9O7NgxrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing Training"
      ],
      "metadata": {
        "id": "Eo52Dghog8l3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 15\n",
        "def training_function(epochs, model, optimizer, criterion, train_dataloader, valid_dataloader):\n",
        "    for epoch in range(1, epochs + 1):\n",
        "     \n",
        "        #set training and valid loss to zero\n",
        "        training_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "        \n",
        "        #set model for train\n",
        "        model.train()\n",
        "        \n",
        "        for batch_idx, batch in enumerate(train_dataloader):\n",
        "            \n",
        "            tweet, label = batch\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            predict = model(tweet)\n",
        "            \n",
        "            loss = criterion(predict, label)\n",
        "            \n",
        "            loss.backward()\n",
        "            \n",
        "            optimizer.step()\n",
        "            \n",
        "            training_loss += loss.data.item() * tweet.size(0)\n",
        "            \n",
        "        training_loss /= len(train_dataloader)\n",
        " \n",
        "        #set model for evalution\n",
        "        model.eval()\n",
        "        for batch_idx,batch in enumerate(valid_dataloader):\n",
        "\n",
        "            tweet, label = batch\n",
        "            \n",
        "            predict = model(tweet)\n",
        "            loss = criterion(predict, label)\n",
        "            valid_loss += loss.data.item() * tweet.size(0)\n",
        " \n",
        "        valid_loss /= len(valid_dataloader)\n",
        "        print('Epoch: {} - loss: {:.2f} - val_ loss: {:.2f}'.format(epoch, training_loss, valid_loss))\n"
      ],
      "metadata": {
        "id": "_N3AHmeBhAHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "1EOX9TsahEW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_function(epochs, model, optimizer, criterion, train_dataloader, valid_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2O3Nq7KhFxJ",
        "outputId": "f9ff0787-234c-4436-8cc5-08f43bb76e6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - loss: 2.6431 - val_loss: 2.6653\n",
            "Epoch 2 - loss: 2.3759 - val_loss: 3.9411\n",
            "Epoch 3 - loss: 2.0834 - val_loss: 7.2338\n",
            "Epoch 4 - loss: 1.8380 - val_loss: 9.4135\n",
            "Epoch 5 - loss: 1.6002 - val_loss: 10.0389\n",
            "Epoch 6 - loss: 1.3725 - val_loss: 11.0042\n",
            "Epoch 7 - loss: 1.1924 - val_loss: 10.2766\n",
            "Epoch 8 - loss: 1.0529 - val_loss: 9.2593\n",
            "Epoch 9 - loss: 0.9137 - val_loss: 9.9668\n",
            "Epoch 10 - loss: 0.7928 - val_loss: 9.4821\n",
            "Epoch 11 - loss: 0.6885 - val_loss: 8.7342\n",
            "Epoch 12 - loss: 0.6094 - val_loss: 8.5325\n",
            "Epoch 13 - loss: 0.5345 - val_loss: 7.9924\n",
            "Epoch 14 - loss: 0.4800 - val_loss: 7.8522\n",
            "Epoch 15 - loss: 0.4357 - val_loss: 7.1004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the Model"
      ],
      "metadata": {
        "id": "2jw8nIzh0ueI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I manually randomly sampled a tweet content data from the dataset."
      ],
      "metadata": {
        "id": "U_OC545B1bUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentimentRecognition(tweet_context):\n",
        "    categories = {0: \"Negative\", 1:\"Positive\"}\n",
        "    processed = TWEET.process([TWEET.preprocess(tweet_context)])\n",
        "    \n",
        "    model.eval()\n",
        "    prediction = model(processed)\n",
        "    print(\"Prediction: \",  prediction)\n",
        "    pred_result = categories[prediction.argmax().item()] \n",
        "    return pred_result"
      ],
      "metadata": {
        "id": "tcAdQMMt0xZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_tweet = \"@SkylineStudio Oh, yes, I'm pretty blessed. AND I love good food so we make a good match!\"\n",
        "sentimentRecognition(test_tweet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVLqIowh08WL",
        "outputId": "a26b8e67-cc81-494e-faec-74a55206ac81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction:  tensor([[ 0.2838, -0.2630]], grad_fn=<AddmmBackward>)\n",
            "'Positive'\n"
          ]
        }
      ]
    }
  ]
}